{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7497f78a",
   "metadata": {},
   "source": [
    "# üîç Apache Spark vs DuckDB: Performance Comparison\n",
    "\n",
    "This notebook benchmarks SQL query execution performance between **Apache Spark** and **DuckDB** on the same datasets in a single-node environment. The objective is to test runtime, usability, and functionality of both engines using real-world JSON data.\n",
    "\n",
    "**Dataset 1**: ADS-B telemetry data from aircraft  \n",
    "**Dataset 2**: Aircraft metadata keyed by `hex` ID\n",
    "\n",
    "We will:\n",
    "- Load both datasets into Spark and DuckDB\n",
    "- Run identical SQL queries\n",
    "- Compare performance and ease of use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa62b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.1\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Confirm environment\n",
    "import platform\n",
    "print(\"Python version:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d0c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "small_telemetry_path = \"D:/Project/Spark_project/combined_file_small.txt\"\n",
    "large_telemetry_path = \"D:/Project/Spark_project/combined_file_large.txt\"\n",
    "aircraft_info_path = \"D:/GitHub/Apache_spark_/spark/basic-ac-db.json\"\n",
    "\n",
    "one_day_path = \"D:/Project/Spark_project/one_day_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fd747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session created\n",
      "‚úÖ DuckDB In-Memory Session initialized\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkDuckDB_Comparison\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session created\")\n",
    "\n",
    "# üê• Initialize DuckDB In-Memory Connection\n",
    "con = duckdb.connect(database=':memory:')\n",
    "print(\"‚úÖ DuckDB In-Memory Session initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2348aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+--------+------------+-----+---------+------+---------+--------+-------+----------------+-----------+---------+----------+---+---+--------+-------+--------+-----+-----+---+--------+---+---+----+----+--------+----+-----+--------+\n",
      "|           timestamp|   hex|alt_baro|alt_geom|ground_speed|track|baro_rate|squawk|emergency|category|nav_qnh|nav_altitude_mcp|nav_heading|      lat|       lon|nic| rc|seen_pos|version|nic_baro|nac_p|nac_v|sil|sil_type|gva|sda|mlat|tisb|messages|seen| rssi|  flight|\n",
      "+--------------------+------+--------+--------+------------+-----+---------+------+---------+--------+-------+----------------+-----------+---------+----------+---+---+--------+-------+--------+-----+-----+---+--------+---+---+----+----+--------+----+-----+--------+\n",
      "|2025-01-31 00:00:...|ab35d3|   37000|   36475|       552.7| 41.3|        0|  5740|     none|      A5| 1013.6|           36992|        0.0|44.218048|-75.741316|  8|186|     0.4|      2|       1|    9|    1|  3| perhour|  2|  2|  []|  []|    1738| 0.1|-17.2|DAL136  |\n",
      "|2025-01-31 00:00:...|c03f37|   23925|   23325|       352.3| 22.7|     -960|  0512|     none|      A3| 1013.6|           14016|        0.0|44.900757| -75.38511|  8|186|     1.4|      2|       1|   10|    2|  3| perhour|  2|  2|  []|  []|    1370| 0.1|-20.5|WJA598  |\n",
      "|2025-01-31 00:00:...|c06a75|   24725|   24125|       482.7| 47.5|    -1856|  1541|     none|      A3| 1012.8|           24000|       NULL|44.762677|-75.105152|  8|186|     0.1|      2|       1|    9|    1|  3| perhour|  2|  3|  []|  []|    3725| 0.0| -4.2|TSC359  |\n",
      "|2025-01-31 00:00:...|c078ba|   15175|   14600|       420.6| 48.1|    -2944|  6107|     none|      A5| 1015.2|            8000|       NULL|45.025955|-74.698661|  8|186|     0.6|      2|       1|    9|    1|  3| perhour|  2|  2|  []|  []|    5081| 0.5| -8.9|TSC939  |\n",
      "|2025-01-31 00:00:...|c027da|   30000|   29450|       343.8|229.1|        0|  0630|     none|      A5| 1013.6|           30016|      257.3|44.001297|-75.327695|  8|186|     0.2|      2|       1|   10|    2|  3| perhour|  2|  2|  []|  []|    6461| 0.0|-16.6|CJT990  |\n",
      "+--------------------+------+--------+--------+------------+-----+---------+------+---------+--------+-------+----------------+-----------+---------+----------+---+---+--------+-------+--------+-----+-----+---+--------+---+---+----+----+--------+----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load full telemetry and extract all payload fields in Spark\n",
    "df_spark_small = spark.read.json(small_telemetry_path)\n",
    "\n",
    "df_spark_small_flat = df_spark_small.select(\n",
    "    col(\"dt\").alias(\"timestamp\"),\n",
    "    col(\"payload.hex\").alias(\"hex\"),\n",
    "    col(\"payload.alt_baro\").alias(\"alt_baro\"),\n",
    "    col(\"payload.alt_geom\").alias(\"alt_geom\"),\n",
    "    col(\"payload.gs\").alias(\"ground_speed\"),\n",
    "    col(\"payload.track\").alias(\"track\"),\n",
    "    col(\"payload.baro_rate\").alias(\"baro_rate\"),\n",
    "    col(\"payload.squawk\").alias(\"squawk\"),\n",
    "    col(\"payload.emergency\").alias(\"emergency\"),\n",
    "    col(\"payload.category\").alias(\"category\"),\n",
    "    col(\"payload.nav_qnh\").alias(\"nav_qnh\"),\n",
    "    col(\"payload.nav_altitude_mcp\").alias(\"nav_altitude_mcp\"),\n",
    "    col(\"payload.nav_heading\").alias(\"nav_heading\"),\n",
    "    col(\"payload.lat\").alias(\"lat\"),\n",
    "    col(\"payload.lon\").alias(\"lon\"),\n",
    "    col(\"payload.nic\").alias(\"nic\"),\n",
    "    col(\"payload.rc\").alias(\"rc\"),\n",
    "    col(\"payload.seen_pos\").alias(\"seen_pos\"),\n",
    "    col(\"payload.version\").alias(\"version\"),\n",
    "    col(\"payload.nic_baro\").alias(\"nic_baro\"),\n",
    "    col(\"payload.nac_p\").alias(\"nac_p\"),\n",
    "    col(\"payload.nac_v\").alias(\"nac_v\"),\n",
    "    col(\"payload.sil\").alias(\"sil\"),\n",
    "    col(\"payload.sil_type\").alias(\"sil_type\"),\n",
    "    col(\"payload.gva\").alias(\"gva\"),\n",
    "    col(\"payload.sda\").alias(\"sda\"),\n",
    "    col(\"payload.mlat\").alias(\"mlat\"),\n",
    "    col(\"payload.tisb\").alias(\"tisb\"),\n",
    "    col(\"payload.messages\").alias(\"messages\"),\n",
    "    col(\"payload.seen\").alias(\"seen\"),\n",
    "    col(\"payload.rssi\").alias(\"rssi\"),\n",
    "    col(\"payload.flight\").alias(\"flight\")\n",
    ")\n",
    "\n",
    "df_spark_small_flat.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, BooleanType\n",
    "\n",
    "# payload_schema = StructType([\n",
    "#     StructField(\"hex\", StringType()),\n",
    "#     StructField(\"alt_baro\", DoubleType()),\n",
    "#     StructField(\"alt_geom\", DoubleType()),\n",
    "#     StructField(\"gs\", DoubleType()),\n",
    "#     StructField(\"track\", DoubleType()),\n",
    "#     StructField(\"baro_rate\", DoubleType()),\n",
    "#     StructField(\"squawk\", StringType()),\n",
    "#     StructField(\"emergency\", StringType()),\n",
    "#     StructField(\"category\", StringType()),\n",
    "#     StructField(\"nav_qnh\", DoubleType()),\n",
    "#     StructField(\"nav_altitude_mcp\", DoubleType()),\n",
    "#     StructField(\"nav_heading\", DoubleType()),\n",
    "#     StructField(\"lat\", DoubleType()),\n",
    "#     StructField(\"lon\", DoubleType()),\n",
    "#     StructField(\"nic\", IntegerType()),\n",
    "#     StructField(\"rc\", IntegerType()),\n",
    "#     StructField(\"seen_pos\", DoubleType()),\n",
    "#     StructField(\"version\", IntegerType()),\n",
    "#     StructField(\"nic_baro\", IntegerType()),\n",
    "#     StructField(\"nac_p\", IntegerType()),\n",
    "#     StructField(\"nac_v\", IntegerType()),\n",
    "#     StructField(\"sil\", IntegerType()),\n",
    "#     StructField(\"sil_type\", StringType()),\n",
    "#     StructField(\"gva\", IntegerType()),\n",
    "#     StructField(\"sda\", IntegerType()),\n",
    "#     StructField(\"mlat\", BooleanType()),\n",
    "#     StructField(\"tisb\", BooleanType()),\n",
    "#     StructField(\"messages\", IntegerType()),\n",
    "#     # StructField(\"seen\", DoubleType()),\n",
    "#     StructField(\"rssi\", DoubleType()),\n",
    "#     StructField(\"flight\", StringType())\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+--------+-----+-----+---------+------+---------+--------+-------+----------------+-----------+---------+----------+---+---+--------+-------+--------+-----+-----+---+--------+---+---+----+----+--------+----+-----+--------+\n",
      "|           timestamp|   hex|alt_baro|alt_geom|   gs|track|baro_rate|squawk|emergency|category|nav_qnh|nav_altitude_mcp|nav_heading|      lat|       lon|nic| rc|seen_pos|version|nic_baro|nac_p|nac_v|sil|sil_type|gva|sda|mlat|tisb|messages|seen| rssi|  flight|\n",
      "+--------------------+------+--------+--------+-----+-----+---------+------+---------+--------+-------+----------------+-----------+---------+----------+---+---+--------+-------+--------+-----+-----+---+--------+---+---+----+----+--------+----+-----+--------+\n",
      "|2024-11-05 00:00:...|407799| 39000.0| 40275.0|537.8| 66.4|      0.0|  3405|     none|      A5| 1013.6|         39008.0|       73.1|44.148699| -74.63974|  8|186|     0.1|      2|       1|   10|    2|  3| perhour|  2|  2|NULL|NULL|    2720| 0.1|-15.4|BAW19W  |\n",
      "|2024-11-05 00:00:...|c045a7| 36000.0| 37050.0|384.0|248.0|    -64.0|  0655|     NULL|      A3| 1013.6|         36000.0|      265.1|45.296165|-76.085079|  8|186|     0.2|      2|       1|   10|    1|  3| perhour|  2|  2|NULL|NULL|    1220| 0.1|-22.3|00000000|\n",
      "|2024-11-05 00:00:...|c00ebb| 11525.0| 11850.0|430.0| 51.6|  -2432.0|  6307|     none|      A4| 1018.4|          8000.0|       61.2|45.014236|-75.159302|  8|186|     0.6|      2|       1|    9|    1|  3| perhour|  2|  2|NULL|NULL|    1610| 0.6|-23.2|CJT626  |\n",
      "|2024-11-05 00:00:...|c0173e| 32925.0| 34000.0|557.3| 70.4|   1216.0|  2240|     none|      A5| 1012.8|         32992.0|       80.9|44.975143|-75.415257|  8|186|     0.8|      2|       1|   10|    2|  3| perhour|  2|  2|NULL|NULL|    1478| 0.8|-21.8|ACA842  |\n",
      "|2024-11-05 00:00:...|407799| 39000.0| 40275.0|538.8| 66.5|      0.0|  3405|     none|      A5| 1013.6|         39008.0|       73.1|44.149584|-74.636928|  8|186|     0.2|      2|       1|   10|    2|  3| perhour|  2|  2|NULL|NULL|    2729| 0.2|-16.7|BAW19W  |\n",
      "+--------------------+------+--------+--------+-----+-----+---------+------+---------+--------+-------+----------------+-----------+---------+----------+---+---+--------+-------+--------+-----+-----+---+--------+---+---+----+----+--------+----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import from_json\n",
    "\n",
    "# df_spark_large_raw = spark.read.json(large_telemetry_path)\n",
    "\n",
    "# df_spark_large = df_spark_large_raw.withColumn(\n",
    "#     \"parsed_payload\", from_json(\"payload\", payload_schema)\n",
    "# ).select(\n",
    "#     col(\"dt\").alias(\"timestamp\"),\n",
    "#     col(\"parsed_payload.*\")  # unpack all fields from payload\n",
    "# )\n",
    "\n",
    "# df_spark_large.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e9373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>dt</th>\n",
       "      <th>payload</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new_adsb</td>\n",
       "      <td>2025-01-31 00:00:00.330843</td>\n",
       "      <td>{'hex': 'ab35d3', 'flight': 'DAL136  ', 'alt_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_adsb</td>\n",
       "      <td>2025-01-31 00:00:00.336607</td>\n",
       "      <td>{'hex': 'c03f37', 'flight': 'WJA598  ', 'alt_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_adsb</td>\n",
       "      <td>2025-01-31 00:00:00.336872</td>\n",
       "      <td>{'hex': 'c06a75', 'flight': 'TSC359  ', 'alt_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_adsb</td>\n",
       "      <td>2025-01-31 00:00:00.337039</td>\n",
       "      <td>{'hex': 'c078ba', 'flight': 'TSC939  ', 'alt_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_adsb</td>\n",
       "      <td>2025-01-31 00:00:00.337196</td>\n",
       "      <td>{'hex': 'c027da', 'flight': 'CJT990  ', 'alt_b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                         dt  \\\n",
       "0  new_adsb 2025-01-31 00:00:00.330843   \n",
       "1  new_adsb 2025-01-31 00:00:00.336607   \n",
       "2  new_adsb 2025-01-31 00:00:00.336872   \n",
       "3  new_adsb 2025-01-31 00:00:00.337039   \n",
       "4  new_adsb 2025-01-31 00:00:00.337196   \n",
       "\n",
       "                                             payload  \n",
       "0  {'hex': 'ab35d3', 'flight': 'DAL136  ', 'alt_b...  \n",
       "1  {'hex': 'c03f37', 'flight': 'WJA598  ', 'alt_b...  \n",
       "2  {'hex': 'c06a75', 'flight': 'TSC359  ', 'alt_b...  \n",
       "3  {'hex': 'c078ba', 'flight': 'TSC939  ', 'alt_b...  \n",
       "4  {'hex': 'c027da', 'flight': 'CJT990  ', 'alt_b...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load full payload fields into DuckDB from small telemetry dataset\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE duck_small_telemetry AS\n",
    "SELECT * \n",
    "FROM read_json_auto('{small_telemetry_path}', union_by_name=true, sample_size=-1);\n",
    "\"\"\")\n",
    "\n",
    "# Preview table\n",
    "con.execute(\"SELECT * FROM duck_small_telemetry LIMIT 5\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f90ca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hex  flights\n",
      "0  89901f      855\n",
      "1  c04463      615\n",
      "2  c00a0b      585\n",
      "3  ab35d3      551\n",
      "4  c02fe7      546\n",
      "5  407f2c      505\n",
      "6  ab7fe6      497\n",
      "7  a7615f      495\n",
      "8  c0202d      491\n",
      "9  a89bec      489\n",
      "‚è± DuckDB query time: 0.2910 seconds\n",
      "‚úÖ DuckDB query executed successfully\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Flatten + group all in one go (inside subquery)\n",
    "result_df = con.sql(\"\"\"\n",
    "SELECT hex, COUNT(*) AS flights\n",
    "FROM (\n",
    "    SELECT \n",
    "        payload ->> 'hex' AS hex\n",
    "    FROM duck_small_telemetry\n",
    "    WHERE payload ->> 'hex' IS NOT NULL\n",
    "    LIMIT 10000\n",
    ")\n",
    "GROUP BY hex\n",
    "ORDER BY flights DESC\n",
    "LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Show result and timing\n",
    "print(result_df)\n",
    "print(f\"‚è± DuckDB query time: {end - start:.4f} seconds\")\n",
    "print(\"‚úÖ DuckDB query executed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd4e3a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hex  count\n",
      "0  89901f    855\n",
      "1  c04463    615\n",
      "2  c00a0b    585\n",
      "3  ab35d3    551\n",
      "4  c02fe7    546\n",
      "5  407f2c    505\n",
      "6  ab7fe6    497\n",
      "7  a7615f    495\n",
      "8  c0202d    491\n",
      "9  a89bec    489\n",
      "‚ö° Spark (small) query time: 1.7658 seconds\n",
      "‚úÖ Spark small dataset query completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "spark_result_small = df_spark_small_flat.limit(10000) \\\n",
    "    .filter(col(\"hex\").isNotNull()) \\\n",
    "    .groupBy(\"hex\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10) \\\n",
    "    .toPandas()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(spark_result_small)\n",
    "print(f\"‚ö° Spark (small) query time: {end - start:.4f} seconds\")\n",
    "print(\"‚úÖ Spark small dataset query completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c4d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       hex  flights\n",
      "8   89901f      837\n",
      "23  c04463      604\n",
      "17  c00a0b      561\n",
      "13  ab35d3      551\n",
      "21  c02fe7      518\n",
      "2   407f2c      505\n",
      "10  a7615f      487\n",
      "14  ab7fe6      468\n",
      "19  c0202d      467\n",
      "11  a89bec      437\n",
      "üêº Pandas (small) query time: 0.0240 seconds\n",
      "‚úÖ Pandas small dataset query completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# Load as line-delimited JSON\n",
    "df_pandas = pd.read_json(small_telemetry_path, lines=True)\n",
    "\n",
    "# Flatten the nested payload column\n",
    "pandas_df = pd.json_normalize(df_pandas['payload'])\n",
    "pandas_df = df_pandas[['dt']].join(pandas_df)\n",
    "pandas_df.rename(columns={'dt': 'timestamp'}, inplace=True)\n",
    "\n",
    "# ‚úÖ Take a 10,000-row sample for the benchmark\n",
    "df_pandas_sample = pandas_df.head(10000)\n",
    "\n",
    "# üïí Run benchmark\n",
    "start = time.time()\n",
    "\n",
    "pandas_result = df_pandas_sample[df_pandas_sample[\"hex\"].notnull()] \\\n",
    "    .groupby(\"hex\")[\"flight\"] \\\n",
    "    .count() \\\n",
    "    .reset_index(name=\"flights\") \\\n",
    "    .sort_values(\"flights\", ascending=False) \\\n",
    "    .head(10)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# ‚úÖ Print result and timing\n",
    "print(pandas_result)\n",
    "print(f\"üêº Pandas (small) query time: {end - start:.4f} seconds\")\n",
    "print(\"‚úÖ Pandas small dataset query completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a1139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hex  flights\n",
      "0  89901f      842\n",
      "1  c00a0b      575\n",
      "2  ab35d3      551\n",
      "3  c02fe7      531\n",
      "4  407f2c      500\n",
      "5  ab7fe6      491\n",
      "6  a7615f      485\n",
      "7  c0202d      478\n",
      "8  48ae03      418\n",
      "9  c04adc      365\n",
      "ü¶Ü DuckDB query 2 time: 0.1820 seconds\n",
      "‚úÖ DuckDB query 2 executed successfully\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "duck_result_q2 = con.sql(\"\"\"\n",
    "SELECT hex, COUNT(*) AS flights\n",
    "FROM (\n",
    "    SELECT \n",
    "        payload ->> 'hex' AS hex,\n",
    "        TRY_CAST(payload ->> 'alt_baro' AS DOUBLE) AS alt_baro,\n",
    "        TRY_CAST(payload ->> 'lat' AS DOUBLE) AS lat\n",
    "    FROM duck_small_telemetry\n",
    "    LIMIT 10000\n",
    ")\n",
    "WHERE alt_baro > 30000 AND lat IS NOT NULL\n",
    "GROUP BY hex\n",
    "ORDER BY flights DESC\n",
    "LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(duck_result_q2)\n",
    "print(f\"ü¶Ü DuckDB query 2 time: {end - start:.4f} seconds\")\n",
    "print(\"‚úÖ DuckDB query 2 executed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63498ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hex  count\n",
      "0  89901f    842\n",
      "1  c00a0b    575\n",
      "2  ab35d3    551\n",
      "3  c02fe7    531\n",
      "4  407f2c    500\n",
      "5  ab7fe6    491\n",
      "6  a7615f    485\n",
      "7  c0202d    478\n",
      "8  48ae03    418\n",
      "9  c04adc    365\n",
      "‚ö° Spark query 2 time: 1.2075 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "spark_result_q2 = df_spark_small_flat.limit(10000) \\\n",
    "    .filter((col(\"alt_baro\") > 30000) & (col(\"lat\").isNotNull()) & (col(\"hex\").isNotNull())) \\\n",
    "    .groupBy(\"hex\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10) \\\n",
    "    .toPandas()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(spark_result_q2)\n",
    "print(f\"‚ö° Spark query 2 time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88d56acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       hex  flights\n",
      "6   89901f      837\n",
      "14  c00a0b      560\n",
      "11  ab35d3      551\n",
      "17  c02fe7      503\n",
      "2   407f2c      500\n",
      "8   a7615f      481\n",
      "12  ab7fe6      468\n",
      "15  c0202d      467\n",
      "3   48ae03      414\n",
      "19  c04adc      332\n",
      "üêº Pandas query 2 time: 0.0334 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pandas_result_q2 = df_pandas_sample[\n",
    "    (df_pandas_sample[\"alt_baro\"] > 30000) & \n",
    "    (df_pandas_sample[\"lat\"].notnull()) & \n",
    "    (df_pandas_sample[\"hex\"].notnull())\n",
    "].groupby(\"hex\")[\"flight\"].count().reset_index(name=\"flights\") \\\n",
    " .sort_values(\"flights\", ascending=False) \\\n",
    " .head(10)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(pandas_result_q2)\n",
    "print(f\"üêº Pandas query 2 time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23898ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [hex, alt_baro, lat, timestamp, rn]\n",
      "Index: []\n",
      "ü¶Ü DuckDB window query time: 1.0099 seconds\n",
      "‚úÖ DuckDB window query executed successfully\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "duck_window_result = con.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT \n",
    "        payload ->> 'hex' AS hex,\n",
    "        TRY_CAST(payload ->> 'alt_baro' AS DOUBLE) AS alt_baro,\n",
    "        TRY_CAST(payload ->> 'lat' AS DOUBLE) AS lat,\n",
    "        dt AS timestamp,\n",
    "        ROW_NUMBER() OVER (PARTITION BY payload ->> 'hex' ORDER BY dt DESC) AS rn\n",
    "    FROM duck_small_telemetry\n",
    "    WHERE TRY_CAST(payload ->> 'alt_baro' AS DOUBLE) > 30000\n",
    "      AND CAST(dt AS DATE) = DATE '2025-01-31'\n",
    "    LIMIT 10000\n",
    ")\n",
    "WHERE rn = 1\n",
    "\"\"\").df()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(duck_window_result)\n",
    "print(f\"ü¶Ü DuckDB window query time: {end - start:.4f} seconds\")\n",
    "print(\"‚úÖ DuckDB window query executed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd99e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        hex alt_baro        lat                   timestamp\n",
      "0    0101db    34975        NaN  2025-01-31 13:43:05.129863\n",
      "1    0201a3    36975  44.483363  2025-01-31 23:03:32.190863\n",
      "2    040198    34250  45.234650  2025-01-31 11:12:25.265773\n",
      "3    06a070    31000  45.250031  2025-01-31 20:52:12.732107\n",
      "4    06a081    31000  44.049217  2025-01-31 10:37:55.798193\n",
      "..      ...      ...        ...                         ...\n",
      "399  c0884f    30375  44.725156  2025-01-31 00:50:46.500880\n",
      "400  c08852    31025  45.811359  2025-01-31 19:02:28.012323\n",
      "401  c08857    36000        NaN  2025-01-31 19:52:20.879995\n",
      "402  c08863    37525        NaN  2025-01-31 17:19:28.746109\n",
      "403  c2b3c3    35000  45.303589  2025-01-31 08:30:26.677906\n",
      "\n",
      "[404 rows x 4 columns]\n",
      "‚ö° Spark window query time: 4.4191 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, to_date\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "window_spec = Window.partitionBy(\"hex\").orderBy(col(\"timestamp\").desc())\n",
    "\n",
    "spark_window_result = df_spark_small_flat \\\n",
    "    .filter((col(\"alt_baro\") > 30000) & \n",
    "            (to_date(\"timestamp\") == \"2025-01-31\") & \n",
    "            (col(\"hex\").isNotNull())) \\\n",
    "    .withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "    .filter(\"rn = 1\") \\\n",
    "    .select(\"hex\", \"alt_baro\", \"lat\", \"timestamp\") \\\n",
    "    .limit(10000) \\\n",
    "    .toPandas()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(spark_window_result)\n",
    "print(f\"‚ö° Spark window query time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f30099f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         hex alt_baro        lat                  timestamp\n",
      "404   0c20a8    30125        NaN 2025-01-31 00:01:27.350301\n",
      "2774  4005c0    37000        NaN 2025-01-31 00:15:42.388500\n",
      "9915  407f2c    39000  45.314850 2025-01-31 01:31:15.377248\n",
      "3068  484416    35000        NaN 2025-01-31 00:19:06.486362\n",
      "6153  48ae03    59100  46.082898 2025-01-31 00:38:44.428345\n",
      "6934  4ba94d    35000        NaN 2025-01-31 00:55:48.785736\n",
      "9998  8406d0    34000  43.537189 2025-01-31 01:44:06.312562\n",
      "8660  89901f    31000  45.604797 2025-01-31 01:13:46.271610\n",
      "1345  a55ddf    45000  43.659760 2025-01-31 00:05:35.076742\n",
      "4953  a7615f    39025  45.373260 2025-01-31 00:28:46.781585\n",
      "4358  a89bec    36000  43.507815 2025-01-31 00:25:51.032281\n",
      "4939  aa6b9f    37000  45.804749 2025-01-31 00:28:38.594846\n",
      "2214  ab35d3    37000  45.354355 2025-01-31 00:11:06.475285\n",
      "4875  ab7fe6    37000  44.497329 2025-01-31 00:27:54.672400\n",
      "9999  c0054d    35000  44.560730 2025-01-31 01:44:06.312725\n",
      "6346  c00a0b    37000        NaN 2025-01-31 00:44:36.595217\n",
      "9335  c0202d    37000  44.896933 2025-01-31 01:21:35.293571\n",
      "375   c027da    30025  43.925354 2025-01-31 00:01:19.167506\n",
      "2452  c02fe7    37000  44.356110 2025-01-31 00:12:34.390682\n",
      "4871  c04463    30050  44.319717 2025-01-31 00:27:53.646871\n",
      "7471  c04adc    40000  45.075322 2025-01-31 01:02:05.622410\n",
      "5171  c04ae8    32900  45.996870 2025-01-31 00:30:54.678195\n",
      "7664  c05d56    30275  44.748001 2025-01-31 01:05:10.323077\n",
      "2168  c07b0a    30225        NaN 2025-01-31 00:10:49.096432\n",
      "6606  c0884f    30375  44.725156 2025-01-31 00:50:46.500880\n",
      "üêº Pandas window query time: 0.0654 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raksh\\AppData\\Local\\Temp\\ipykernel_5032\\810727891.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pandas_sample[\"timestamp\"] = pd.to_datetime(df_pandas_sample[\"timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Convert to datetime if not already\n",
    "df_pandas_sample[\"timestamp\"] = pd.to_datetime(df_pandas_sample[\"timestamp\"])\n",
    "\n",
    "pandas_window_result = df_pandas_sample[\n",
    "    (df_pandas_sample[\"alt_baro\"] > 30000) &\n",
    "    (df_pandas_sample[\"timestamp\"].dt.date == pd.to_datetime(\"2025-01-31\").date()) &\n",
    "    (df_pandas_sample[\"hex\"].notnull())\n",
    "].sort_values([\"hex\", \"timestamp\"], ascending=[True, False]) \\\n",
    " .drop_duplicates(subset=\"hex\", keep=\"first\") \\\n",
    " .loc[:, [\"hex\", \"alt_baro\", \"lat\", \"timestamp\"]] \\\n",
    " .head(10000)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(pandas_window_result)\n",
    "print(f\"üêº Pandas window query time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60245bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hex  alt_baro                  timestamp\n",
      "0  ab35d3   37000.0 2025-01-31 00:00:00.330843\n",
      "1  c03f37   23925.0 2025-01-31 00:00:00.336607\n",
      "2  c06a75   24725.0 2025-01-31 00:00:00.336872\n",
      "3  c078ba   15175.0 2025-01-31 00:00:00.337039\n",
      "4  c027da   30000.0 2025-01-31 00:00:00.337196\n",
      "ü¶Ü DuckDB date range filter time: 0.1202 seconds\n",
      "‚úÖ DuckDB date range filter executed successfully\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "duck_date_range_result = con.sql(\"\"\"\n",
    "SELECT \n",
    "    payload ->> 'hex' AS hex,\n",
    "    TRY_CAST(payload ->> 'alt_baro' AS DOUBLE) AS alt_baro,\n",
    "    dt AS timestamp\n",
    "FROM duck_small_telemetry\n",
    "WHERE CAST(dt AS DATE) BETWEEN DATE '2025-01-15' AND DATE '2025-01-31'\n",
    "LIMIT 10000\n",
    "\"\"\").df()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(duck_date_range_result.head())\n",
    "print(f\"ü¶Ü DuckDB date range filter time: {end - start:.4f} seconds\")\n",
    "print(\"‚úÖ DuckDB date range filter executed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "191a9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hex alt_baro                   timestamp\n",
      "0  ab35d3    37000  2025-01-31 00:00:00.330843\n",
      "1  c03f37    23925  2025-01-31 00:00:00.336607\n",
      "2  c06a75    24725  2025-01-31 00:00:00.336872\n",
      "3  c078ba    15175  2025-01-31 00:00:00.337039\n",
      "4  c027da    30000  2025-01-31 00:00:00.337196\n",
      "‚ö° Spark date range filter time: 0.3030 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "spark_date_range_result = df_spark_small_flat \\\n",
    "    .filter(\n",
    "        (to_date(\"timestamp\") >= \"2025-01-15\") & \n",
    "        (to_date(\"timestamp\") <= \"2025-01-31\")\n",
    "    ) \\\n",
    "    .select(\"hex\", \"alt_baro\", \"timestamp\") \\\n",
    "    .limit(10000) \\\n",
    "    .toPandas()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(spark_date_range_result.head())\n",
    "print(f\"‚ö° Spark date range filter time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b57ae6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hex alt_baro                  timestamp\n",
      "0  ab35d3    37000 2025-01-31 00:00:00.330843\n",
      "1  c03f37    23925 2025-01-31 00:00:00.336607\n",
      "2  c06a75    24725 2025-01-31 00:00:00.336872\n",
      "3  c078ba    15175 2025-01-31 00:00:00.337039\n",
      "4  c027da    30000 2025-01-31 00:00:00.337196\n",
      "üêº Pandas date range filter time: 0.0126 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raksh\\AppData\\Local\\Temp\\ipykernel_5032\\2342127336.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pandas_sample[\"timestamp\"] = pd.to_datetime(df_pandas_sample[\"timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "# Ensure timestamp is in datetime format\n",
    "df_pandas_sample[\"timestamp\"] = pd.to_datetime(df_pandas_sample[\"timestamp\"])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pandas_date_range_result = df_pandas_sample[\n",
    "    (df_pandas_sample[\"timestamp\"].dt.date >= pd.to_datetime(\"2025-01-15\").date()) &\n",
    "    (df_pandas_sample[\"timestamp\"].dt.date <= pd.to_datetime(\"2025-01-31\").date())\n",
    "][[\"hex\", \"alt_baro\", \"timestamp\"]].head(10000)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(pandas_date_range_result.head())\n",
    "print(f\"üêº Pandas date range filter time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34730f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  total_records\n",
      "0 2025-01-31         301388\n",
      "1 2025-02-01         305422\n",
      "2 2025-02-02         358110\n",
      "3 2025-02-03         263173\n",
      "4 2025-02-04         282303\n",
      "5 2025-02-05         319251\n",
      "ü¶Ü DuckDB date-wise count time: 0.0261 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "duck_count_by_date = con.sql(\"\"\"\n",
    "SELECT \n",
    "    CAST(dt AS DATE) AS date,\n",
    "    COUNT(*) AS total_records\n",
    "FROM duck_small_telemetry\n",
    "GROUP BY date\n",
    "ORDER BY date\n",
    "\"\"\").df()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(duck_count_by_date)\n",
    "print(f\"ü¶Ü DuckDB date-wise count time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aabe136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date   count\n",
      "0  2025-01-31  301388\n",
      "1  2025-02-01  305422\n",
      "2  2025-02-02  358110\n",
      "3  2025-02-03  263173\n",
      "4  2025-02-04  282303\n",
      "5  2025-02-05  319251\n",
      "‚ö° Spark date-wise count time: 1.2455 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "spark_count_by_date = df_spark_small_flat \\\n",
    "    .withColumn(\"date\", to_date(\"timestamp\")) \\\n",
    "    .groupBy(\"date\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"date\") \\\n",
    "    .toPandas()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(spark_count_by_date)\n",
    "print(f\"‚ö° Spark date-wise count time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e114c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  total_records\n",
      "0  2025-01-31          10000\n",
      "üêº Pandas date-wise count time: 0.0125 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raksh\\AppData\\Local\\Temp\\ipykernel_5032\\2517065415.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pandas_sample[\"timestamp\"] = pd.to_datetime(df_pandas_sample[\"timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "# Make sure timestamp is datetime\n",
    "df_pandas_sample[\"timestamp\"] = pd.to_datetime(df_pandas_sample[\"timestamp\"])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pandas_count_by_date = df_pandas_sample.copy()\n",
    "pandas_count_by_date[\"date\"] = pandas_count_by_date[\"timestamp\"].dt.date\n",
    "\n",
    "pandas_count_by_date = pandas_count_by_date \\\n",
    "    .groupby(\"date\")[\"hex\"] \\\n",
    "    .count() \\\n",
    "    .reset_index(name=\"total_records\") \\\n",
    "    .sort_values(\"date\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(pandas_count_by_date)\n",
    "print(f\"üêº Pandas date-wise count time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ea10a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           hex    flight\n",
      "0       c02fe7  ACA620  \n",
      "1       c02fe7  ACA620  \n",
      "2       c02fe7  ACA620  \n",
      "3       c02fe7  ACA620  \n",
      "4       c02fe7  ACA620  \n",
      "...        ...       ...\n",
      "397149  440b9c  ABB278  \n",
      "397150  440b9c  ABB278  \n",
      "397151  440b9c  ABB278  \n",
      "397152  440b9c  ABB278  \n",
      "397153  440b9c  ABB278  \n",
      "\n",
      "[397154 rows x 2 columns]\n",
      "ü¶Ü DuckDB pattern match time: 1.9027 sec\n"
     ]
    }
   ],
   "source": [
    "#pattern match\n",
    "\n",
    "start = time.time()\n",
    "duck_pattern = con.sql(\"\"\"\n",
    "SELECT hex, flight\n",
    "FROM (\n",
    "    SELECT payload ->> 'hex' AS hex, payload ->> 'flight' AS flight\n",
    "    FROM duck_small_telemetry\n",
    ")\n",
    "WHERE flight LIKE 'A%'\n",
    "\"\"\").df()\n",
    "end = time.time()\n",
    "print(duck_pattern)\n",
    "print(f\"ü¶Ü DuckDB pattern match time: {end - start:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db3621a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Spark pattern match time: 5.4441 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "spark_pattern = df_spark_small_flat.filter(col(\"flight\").startswith(\"A\")).select(\"hex\", \"flight\").toPandas()\n",
    "end = time.time()\n",
    "print(f\"‚ö° Spark pattern match time: {end - start:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9b476c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a662a841f849bb8b344387e02fe583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶Ü DuckDB nested subquery time: 3.4702 sec\n"
     ]
    }
   ],
   "source": [
    "#nested sub query \n",
    "\n",
    "start = time.time()\n",
    "duck_nested = con.sql(\"\"\"\n",
    "SELECT hex, alt_baro FROM (\n",
    "    SELECT payload ->> 'hex' AS hex, TRY_CAST(payload ->> 'alt_baro' AS DOUBLE) AS alt_baro\n",
    "    FROM duck_small_telemetry\n",
    ")\n",
    "WHERE alt_baro > (\n",
    "    SELECT AVG(TRY_CAST(payload ->> 'alt_baro' AS DOUBLE)) FROM duck_small_telemetry\n",
    ")\n",
    "\"\"\").df()\n",
    "end = time.time()\n",
    "print(f\"ü¶Ü DuckDB nested subquery time: {end - start:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "avg_alt = df_spark_small_flat.selectExpr(\"avg(alt_baro)\").first()[0]\n",
    "spark_nested = df_spark_small_flat.filter(col(\"alt_baro\") > avg_alt).select(\"hex\", \"alt_baro\").toPandas()\n",
    "end = time.time()\n",
    "print(f\"‚ö° Spark nested subquery time: {end - start:.4f} sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
